{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#another subset of random edges removed - to see if the results are consistent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pickle as pkl\n",
    "import torch\n",
    "from scipy.spatial.distance import cosine, euclidean\n",
    "from scipy import stats\n",
    "import pandas as pd\n",
    "import pprint\n",
    "pp = pprint.PrettyPrinter(indent=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import seaborn as sns\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import networkx as nx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import operator\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import bz2\n",
    "import _pickle as cPickle\n",
    "\n",
    "# Load any compressed pickle file\n",
    "def decompress_pickle(file):\n",
    "    assert file.endswith(\"pbz2\")\n",
    "    data = bz2.BZ2File(file, \"rb\")\n",
    "    data = cPickle.load(data,encoding=\"latin1\")\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "version_no = \"2\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "dname = \"./../../data_hin/PubMed_orig/\"\n",
    "data_dir = \"./../../data/pubmed_heuristic/\"\n",
    "out_dir = \"./../../out_clust/pubmed_heuristic/version_\"+version_no+\"/cc/\"\n",
    "\n",
    "#\n",
    "out_dir_min_max = out_dir+\"PubMed_da_link/\"\n",
    "#out_dir_min = dname+\"filter_min_edges/\"\n",
    "# out_dir_rand1 = dname_mat+\"filter_rand1_max_edges/\" #matches max edge removal count - remove from only the matrices in the CC\n",
    "# out_dir_rand2 = dname_mat+\"filter_rand2_const_edges/\" #fixed num of edges removal - remove from only the matrices in the CC\n",
    "# out_dir_rand3 = dname_mat+\"filter_rand3_min_edges/\" #matches min edge removal count - remove from only the matrices in the CC\n",
    "#\n",
    "# out_dir_rand4 = dname_mat+\"filter_rand4_max_edges_all_mat/\" #matches max edge removal count, but remove from all the 10 matrices randomly and NOT remove from only the matrices in the CC\n",
    "# out_dir_rand5 = dname_mat+\"filter_rand5_min_edges_all_mat/\"\n",
    "#\n",
    "out_dir_rand8 = out_dir+\"PubMed_rand_link/\" #same as prev two, but distibte the edges count to all matrices uniformly\n",
    "#out_dir_rand7 = dname+\"filter_rand7_min_uniform/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not os.path.exists(out_dir_min_max):\n",
    "    os.makedirs(out_dir_min_max)\n",
    "#\n",
    "# if not os.path.exists(out_dir_max):\n",
    "#     os.makedirs(out_dir_max)\n",
    "# #\n",
    "# if not os.path.exists(out_dir_min):\n",
    "#     os.makedirs(out_dir_min)\n",
    "#\n",
    "# if not os.path.exists(out_dir_rand1):\n",
    "#     os.makedirs(out_dir_rand1)\n",
    "# #\n",
    "# if not os.path.exists(out_dir_rand2):\n",
    "#     os.makedirs(out_dir_rand2)\n",
    "# #\n",
    "# if not os.path.exists(out_dir_rand3):\n",
    "#     os.makedirs(out_dir_rand3)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# #\n",
    "# if not os.path.exists(out_dir_rand4):\n",
    "#     os.makedirs(out_dir_rand4)\n",
    "# #\n",
    "# if not os.path.exists(out_dir_rand5):\n",
    "#     os.makedirs(out_dir_rand5)\n",
    "# #\n",
    "# if not os.path.exists(out_dir_rand6):\n",
    "#     os.makedirs(out_dir_rand6)\n",
    "#\n",
    "if not os.path.exists(out_dir_rand8):\n",
    "    os.makedirs(out_dir_rand8)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "fname_node_label_train = \"label.dat\"\n",
    "fname_node_label_test = \"label.dat.test\"\n",
    "fname_link = \"link.dat\" \n",
    "fname_link_test = \"link.dat.test\"\n",
    "fname_node = \"node.dat\"\n",
    "#fname_all_data_dict = data_dir + \"pubmed_heuristic_data_dict_v\"+version_no+\"_p2.pkl\"\n",
    "fname_all_data_dict = data_dir + \"pubmed_heuristic_data_dict_v\"+version_no+\"_p2.pkl.pbz2\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'./../../data_hin/PubMed_orig/label.dat'"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dname+fname_node_label_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ragu/anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py:1: ParserWarning: Falling back to the 'python' engine because the 'c' engine does not support regex separators (separators > 1 char and different from '\\s+' are interpreted as regex); you can avoid this warning by specifying engine='python'.\n",
      "  \"\"\"Entry point for launching an IPython kernel.\n"
     ]
    }
   ],
   "source": [
    "df_node_label_train = pd.read_csv(open(dname +fname_node_label_train),sep=\"\\s\",header=None)\n",
    "df_node_label_train.columns = [\"node_id\", \"node_name\", \"node_type\", \"node_label\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ragu/anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py:1: ParserWarning: Falling back to the 'python' engine because the 'c' engine does not support regex separators (separators > 1 char and different from '\\s+' are interpreted as regex); you can avoid this warning by specifying engine='python'.\n",
      "  \"\"\"Entry point for launching an IPython kernel.\n"
     ]
    }
   ],
   "source": [
    "df_node_label_test = pd.read_csv(open(dname +fname_node_label_test),sep=\"\\s\",header=None)\n",
    "df_node_label_test.columns = [\"node_id\", \"node_name\", \"node_type\", \"node_label\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ragu/anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py:1: ParserWarning: Falling back to the 'python' engine because the 'c' engine does not support regex separators (separators > 1 char and different from '\\s+' are interpreted as regex); you can avoid this warning by specifying engine='python'.\n",
      "  \"\"\"Entry point for launching an IPython kernel.\n"
     ]
    }
   ],
   "source": [
    "df_link = pd.read_csv(open(dname+fname_link),sep=\"\\s\",header=None)\n",
    "df_link.columns = [\"start_node_id\", \"end_node_id\", \"link_type\", \"link_weight\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ragu/anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py:1: ParserWarning: Falling back to the 'python' engine because the 'c' engine does not support regex separators (separators > 1 char and different from '\\s+' are interpreted as regex); you can avoid this warning by specifying engine='python'.\n",
      "  \"\"\"Entry point for launching an IPython kernel.\n"
     ]
    }
   ],
   "source": [
    "df_link_full = pd.read_csv(open(dname+fname_link),sep=\"\\s\",header=None)\n",
    "df_link_full.columns = [\"start_node_id\", \"end_node_id\", \"link_type\", \"link_weight\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ragu/anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py:1: ParserWarning: Falling back to the 'python' engine because the 'c' engine does not support regex separators (separators > 1 char and different from '\\s+' are interpreted as regex); you can avoid this warning by specifying engine='python'.\n",
      "  \"\"\"Entry point for launching an IPython kernel.\n"
     ]
    }
   ],
   "source": [
    "df_link_test = pd.read_csv(open(dname+fname_link_test),sep=\"\\s\",header=None)\n",
    "df_link_test.columns = [\"start_node_id\", \"end_node_id\", \"link_status\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ragu/anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py:1: ParserWarning: Falling back to the 'python' engine because the 'c' engine does not support regex separators (separators > 1 char and different from '\\s+' are interpreted as regex); you can avoid this warning by specifying engine='python'.\n",
      "  \"\"\"Entry point for launching an IPython kernel.\n"
     ]
    }
   ],
   "source": [
    "df_node = pd.read_csv(open(dname+fname_node),sep=\"\\s\",header=None)\n",
    "df_node.columns = [\"node_id\", \"node_name\", \"node_type\",\"node_attrs\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "#all_data_dict = pkl.load(open(fname_all_data_dict,\"rb\"))\n",
    "all_data_dict = data_dict = decompress_pickle(fname_all_data_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "list_x_id_new = list(all_data_dict[\"matrices\"].keys())\n",
    "list_e_id_new = list(all_data_dict[\"metadata\"][\"dict_e_size\"].keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "dict_map_xid = {'X11': \"X0\",\n",
    " 'X12': \"X1\",\n",
    " 'X22': \"X2\",\n",
    " 'X31': \"X3\",\n",
    " 'X32': \"X4\",\n",
    " 'X33': \"X5\",\n",
    " 'X34': \"X6\",\n",
    " 'X41': \"X7\",\n",
    " 'X42': \"X8\",\n",
    " 'X44': \"X9\"}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "dict_map_eid = {\n",
    "    \"e1\":\"e0\",\n",
    "    \"e2\":\"e1\",\n",
    "    \"e3\":\"e2\",\n",
    "    \"e4\":\"e3\"\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "list_x_id = []\n",
    "for x_id in list_x_id_new:\n",
    "    list_x_id.append(dict_map_xid[x_id])\n",
    "#\n",
    "list_e_id = []\n",
    "for e_id in list_e_id_new:\n",
    "    list_e_id.append(dict_map_eid[e_id])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "#"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fname_prefix = \"X\"\n",
    "# fname_suffix = \".npy\"\n",
    "# fname = dname_mat "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "fname_cc_info = out_dir + \"dict_path_cc_info.pkl\" #<< from the CC selection\n",
    "#fname_cc_select = dname_mat + + \"dict_path_cc_select_info.pkl\" #<< from the CC selection\n",
    "fname_cc_edges_select =  out_dir + \"dict_path_cc_select_info_edges.pkl\" #<< from the CC selection\n",
    "##fname_map_matidx_nodeid = dname_mat +\"id_idx.csv\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading the matrices: \n",
      "x_id:  X4  X.shape:  (5660, 4405)\n",
      "x_id:  X5  X.shape:  (5660, 5660)\n",
      "x_id:  X3  X.shape:  (5660, 2634)\n",
      "x_id:  X7  X.shape:  (608, 2634)\n",
      "x_id:  X6  X.shape:  (5660, 608)\n",
      "x_id:  X8  X.shape:  (608, 4405)\n",
      "x_id:  X9  X.shape:  (608, 608)\n",
      "x_id:  X0  X.shape:  (2634, 2634)\n",
      "x_id:  X1  X.shape:  (2634, 4405)\n",
      "x_id:  X2  X.shape:  (4405, 4405)\n",
      "#\n"
     ]
    }
   ],
   "source": [
    "num_matrices = 10\n",
    "print(\"Loading the matrices: \")\n",
    "data_dict = {}\n",
    "for x_id in all_data_dict[\"matrices\"].keys():\n",
    "    new_x_id = dict_map_xid[x_id]\n",
    "    data_dict[new_x_id] = all_data_dict[\"matrices\"][x_id]\n",
    "    print(\"x_id: \",new_x_id,\" X.shape: \",data_dict[new_x_id].shape)\n",
    "print(\"#\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['X4', 'X5', 'X3', 'X7', 'X6', 'X8', 'X9', 'X0', 'X1', 'X2'])"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_dict.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x_id:  X4 , %nz:  99.79048062152309\n",
      "x_id:  X5 , %nz:  99.96037533244265\n",
      "x_id:  X3 , %nz:  99.86735030626947\n",
      "x_id:  X7 , %nz:  99.90621128561723\n",
      "x_id:  X6 , %nz:  99.93110121815138\n",
      "x_id:  X8 , %nz:  99.84964006213035\n",
      "x_id:  X9 , %nz:  99.96050467451524\n",
      "x_id:  X0 , %nz:  99.87630362602472\n",
      "x_id:  X1 , %nz:  99.6920390561909\n",
      "x_id:  X2 , %nz:  99.86215231118285\n"
     ]
    }
   ],
   "source": [
    "for x_id in data_dict:\n",
    "    if x_id.startswith(\"X\"):\n",
    "        temp_x = data_dict[x_id]\n",
    "        print(\"x_id: \",x_id,\", %nz: \",100 - ((np.sum(temp_x) / np.prod(temp_x.shape)) * 100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['dict_e_id_to_idx_map', 'dict_e_idx_to_id_map', 'x_meta', 'dict_e_size'])"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_data_dict[\"metadata\"].keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['e4', 'e1', 'e3', 'e2'])"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_data_dict[\"metadata\"][\"dict_e_id_to_idx_map\"].keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mode_xid(dict_with_key_xid):\n",
    "    dict_temp = {}\n",
    "    for xid in dict_with_key_xid.keys():\n",
    "        temp_xid = dict_map_xid[xid].replace(\"X\",\"\")\n",
    "        dict_temp[temp_xid] = dict_with_key_xid[xid]\n",
    "    return dict_temp\n",
    "\n",
    "# def mode_eid_xid(dict_with_key_eid_xid):\n",
    "#     dict_temp = {}\n",
    "#     for eid_xid in dict_with_key_eid_xid.keys():\n",
    "#         eid = eid_xid.split(\"_\")[0]\n",
    "#         xid = eid_xid.split(\"_\")[1]\n",
    "#         temp_xid = dict_map_xid[xid]\n",
    "#         temp_eid = dict_map_eid[eid]\n",
    "#         temp_eid_xid = temp_eid+\"_\"+temp_xid\n",
    "#         dict_temp[temp_eid_xid] = dict_with_key_eid_xid[eid_xid]\n",
    "#     return dict_temp\n",
    "\n",
    "def mode_eid(dict_with_key_eid):\n",
    "    dict_temp = {}\n",
    "    for eid in dict_with_key_eid.keys():\n",
    "        temp_eid = int(dict_map_eid[eid].replace(\"e\",\"\"))\n",
    "        dict_temp[temp_eid] = dict_with_key_eid[eid]\n",
    "    return dict_temp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "dict_map_nodeid_matidx = all_data_dict[\"metadata\"][\"dict_e_id_to_idx_map\"]\n",
    "dict_map_matidx_nodeid = all_data_dict[\"metadata\"][\"dict_e_idx_to_id_map\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['e4', 'e1', 'e3', 'e2'])"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dict_map_nodeid_matidx.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['e4', 'e1', 'e3', 'e2'])"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dict_map_matidx_nodeid.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "dict_map_nodeid_matidx = mode_eid(dict_map_nodeid_matidx)\n",
    "dict_map_matidx_nodeid = mode_eid(dict_map_matidx_nodeid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys([3, 0, 2, 1])"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dict_map_nodeid_matidx.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys([3, 0, 2, 1])"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dict_map_matidx_nodeid.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_map_matidx_nodeid = pd.read_csv(fname_map_matidx_nodeid, header=None)\n",
    "# df_map_matidx_nodeid.columns = [\"node_id\",\"idx\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "dict_cc_info = pkl.load(open(fname_cc_info,\"rb\"))\n",
    "#dict_cc_select = pkl.load(open(fname_cc_select,\"rb\"))\n",
    "dict_cc_edges_select = pkl.load(open(fname_cc_edges_select,\"rb\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_map_matidx_nodeid[:3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{(2, 0): {'chain_template_id': 0,\n",
       "  'cur_chain_template_e': {'p1': ['e2', 'e1'], 'p2': ['e2', 'e0', 'e1']},\n",
       "  'cur_chain_template_x': {'p1': 'X4', 'p2': ['X3', 'X1']},\n",
       "  'path_id': 2},\n",
       " (2, 1): {'chain_template_id': 1,\n",
       "  'cur_chain_template_e': {'p1': ['e2', 'e1'], 'p2': ['e2', 'e3', 'e0', 'e1']},\n",
       "  'cur_chain_template_x': {'p1': 'X4', 'p2': ['X6', 'X7', 'X1']},\n",
       "  'path_id': 2},\n",
       " (2, 2): {'chain_template_id': 2,\n",
       "  'cur_chain_template_e': {'p1': ['e2', 'e1'], 'p2': ['e2', 'e3', 'e1']},\n",
       "  'cur_chain_template_x': {'p1': 'X4', 'p2': ['X6', 'X8']},\n",
       "  'path_id': 2},\n",
       " (3, 0): {'chain_template_id': 0,\n",
       "  'cur_chain_template_e': {'p1': ['e3', 'e1'], 'p2': ['e3', 'e0', 'e1']},\n",
       "  'cur_chain_template_x': {'p1': 'X8', 'p2': ['X7', 'X1']},\n",
       "  'path_id': 3}}"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dict_cc_info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_link_copy1 = df_link.copy() #max\n",
    "df_link_copy2 = None\n",
    "#df_link_copy2 = df_link.copy() #min\n",
    "#\n",
    "# df_link_copy3 = df_link.copy() #rand1-max\n",
    "# df_link_copy4 = df_link.copy() #rand2 -const\n",
    "# df_link_copy5 = df_link.copy() #rand3-min\n",
    "#\n",
    "df_link_copy6 = df_link.copy() #rand7-max\n",
    "df_link_copy7 = None\n",
    "#df_link_copy7 = df_link.copy() #rand5-min\n",
    "#df_link_copy8 = df_link.copy() #rand6-const"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "# #update df_map_matidx_nodeid with node_type\n",
    "# dict_temp_nt = {}\n",
    "# for idx,row in df_map_matidx_nodeid.iterrows():\n",
    "#     cur_id = row[\"node_id\"]\n",
    "#     cur_idx = row[\"idx\"]\n",
    "#     df_temp = df_node[df_node[\"node_id\"].isin([cur_id])]\n",
    "#     assert df_temp.shape[0] == 1\n",
    "#     cur_node_type = df_temp[\"node_type\"].item()\n",
    "#     dict_temp_nt[idx] = {\n",
    "#         \"node_id\":cur_id,\n",
    "#         \"idx\":cur_idx,\n",
    "#         \"node_type\":cur_node_type\n",
    "#     }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_map_matidx_nodeid_new = pd.DataFrame(dict_temp_nt).T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_map_matidx_nodeid_new.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_map_matidx_nodeid_new.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_map_matidx_nodeid_new.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_map_matidx_nodeid = df_map_matidx_nodeid_new[[\"node_id\", \"node_type\",\"idx\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_map_matidx_nodeid[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "len(dict_subset_start_count_orig):  11589\n",
      "len(dict_subset_end_count_orig):  10168\n"
     ]
    }
   ],
   "source": [
    "dict_subset_start_count_orig = df_link.groupby(\"start_node_id\").agg([\"count\"])[\"end_node_id\"].to_dict()[\"count\"]\n",
    "dict_subset_end_count_orig = df_link.groupby(\"end_node_id\").agg([\"count\"])[\"start_node_id\"].to_dict()[\"count\"]\n",
    "print(\"len(dict_subset_start_count_orig): \",len(dict_subset_start_count_orig))\n",
    "print(\"len(dict_subset_end_count_orig): \",len(dict_subset_end_count_orig))        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert df_node.shape[0] == np.unique(list(dict_subset_start_count_orig.keys()) + list(dict_subset_end_count_orig.keys())).shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "############################################## max #####################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "dict_subset_start_count = dict_subset_start_count_orig.copy()\n",
    "dict_subset_end_count = dict_subset_end_count_orig.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "##################\n",
      "min_or_max:  max\n",
      "BEFORE df_link.shape:  (40189, 4)\n",
      "##################\n",
      "path_id:  (2, 0)\n",
      "###\n",
      "x_id:  X4\n",
      "df_link_subset.shape:  (2, 4)\n",
      "count:  1\n",
      "#\n",
      "x_id:  X3\n",
      "df_link_subset.shape:  (308, 4)\n",
      "count:  263\n",
      "#\n",
      "x_id:  X1\n",
      "df_link_subset.shape:  (7, 4)\n",
      "count:  7\n",
      "#\n",
      "######\n",
      "before df_link.shape:  (40189, 4)\n",
      "count_all:  271\n",
      "count_all_in:  317\n",
      "after df_link.shape:  (39918, 4)\n",
      "#######\n",
      "path_id:  (2, 1)\n",
      "###\n",
      "x_id:  X4\n",
      "df_link_subset.shape:  (1, 4)\n",
      "count:  0\n",
      "#\n",
      "x_id:  X6\n",
      "df_link_subset.shape:  (50, 4)\n",
      "count:  23\n",
      "#\n",
      "x_id:  X7\n",
      "df_link_subset.shape:  (2, 4)\n",
      "count:  1\n",
      "#\n",
      "x_id:  X1\n",
      "df_link_subset.shape:  (0, 4)\n",
      "count:  0\n",
      "#\n",
      "######\n",
      "before df_link.shape:  (39918, 4)\n",
      "count_all:  24\n",
      "count_all_in:  53\n",
      "after df_link.shape:  (39894, 4)\n",
      "#######\n",
      "path_id:  (2, 2)\n",
      "###\n",
      "x_id:  X4\n",
      "df_link_subset.shape:  (1, 4)\n",
      "count:  0\n",
      "#\n",
      "x_id:  X6\n",
      "df_link_subset.shape:  (27, 4)\n",
      "count:  0\n",
      "#\n",
      "x_id:  X8\n",
      "df_link_subset.shape:  (0, 4)\n",
      "count:  0\n",
      "#\n",
      "######\n",
      "before df_link.shape:  (39894, 4)\n",
      "count_all:  0\n",
      "count_all_in:  28\n",
      "after df_link.shape:  (39894, 4)\n",
      "#######\n",
      "path_id:  (3, 0)\n",
      "###\n",
      "x_id:  X8\n",
      "df_link_subset.shape:  (0, 4)\n",
      "count:  0\n",
      "#\n",
      "x_id:  X7\n",
      "df_link_subset.shape:  (8, 4)\n",
      "count:  6\n",
      "#\n",
      "x_id:  X1\n",
      "df_link_subset.shape:  (3, 4)\n",
      "count:  2\n",
      "#\n",
      "######\n",
      "before df_link.shape:  (39894, 4)\n",
      "count_all:  8\n",
      "count_all_in:  11\n",
      "after df_link.shape:  (39886, 4)\n",
      "#######\n",
      "##################\n",
      "After df_link.shape:  (39886, 4)\n",
      "count_path_all_in:  409\n",
      "count_path_all:  303\n",
      "#\n",
      "Total removed:  303\n",
      "##################\n"
     ]
    }
   ],
   "source": [
    "dict_count_max = {}\n",
    "#\n",
    "df_link = df_link_copy1\n",
    "min_or_max = \"max\"\n",
    "print(\"##################\")\n",
    "print(\"min_or_max: \",min_or_max)\n",
    "print(\"BEFORE df_link.shape: \",df_link.shape)\n",
    "print(\"##################\")\n",
    "count_path_all = 0\n",
    "count_path_all_in = 0\n",
    "prev_count = df_link.shape[0]\n",
    "for path_id in dict_cc_edges_select:\n",
    "    dict_count_max[path_id] = {}\n",
    "    print(\"path_id: \",path_id)\n",
    "    print(\"###\")\n",
    "    cur_path_dict = dict_cc_edges_select[path_id][min_or_max]\n",
    "    #\n",
    "    #list_edges_to_filter = []\n",
    "    #list_nodes = []\n",
    "    list_df = []\n",
    "    count_all = 0\n",
    "    count_all_in = 0\n",
    "    for x_id in cur_path_dict:\n",
    "        print(\"x_id: \",x_id)\n",
    "        cur_X = data_dict[x_id]\n",
    "        cur_X_dict = cur_path_dict[x_id]\n",
    "        col_e_id = cur_X_dict[\"col_e_id\"]\n",
    "        col_node_type = int(col_e_id.replace(\"e\",\"\"))\n",
    "        row_e_id = cur_X_dict[\"row_e_id\"]\n",
    "        row_node_type = int(row_e_id.replace(\"e\",\"\"))\n",
    "        col_idx_list = list(cur_X_dict[\"col_idx_list\"])\n",
    "        row_idx_list = list(cur_X_dict[\"row_idx_list\"])\n",
    "        #obtain the corresponding row and column node ids\n",
    "        #df_row_nodes = df_map_matidx_nodeid[df_map_matidx_nodeid[\"node_type\"].isin([row_node_type])]\n",
    "        #list_row_node_id_list = df_row_nodes[df_row_nodes[\"idx\"].isin(row_idx_list)][\"node_id\"].unique().tolist()\n",
    "        list_row_node_id_list = []\n",
    "        for temp_idx in row_idx_list:\n",
    "            list_row_node_id_list.append(dict_map_matidx_nodeid[row_node_type][temp_idx])\n",
    "        #df_col_nodes = df_map_matidx_nodeid[df_map_matidx_nodeid[\"node_type\"].isin([col_node_type])]\n",
    "        #list_col_node_id_list = df_col_nodes[df_col_nodes[\"idx\"].isin(col_idx_list)][\"node_id\"].unique().tolist()\n",
    "        list_col_node_id_list = []\n",
    "        for temp_idx in col_idx_list:\n",
    "            list_col_node_id_list.append(dict_map_matidx_nodeid[col_node_type][temp_idx])\n",
    "        #\n",
    "        df_link_subset = df_link[df_link[\"start_node_id\"].isin(list_row_node_id_list)\\\n",
    "                                 & df_link[\"end_node_id\"].isin(list_col_node_id_list)]\n",
    "        print(\"df_link_subset.shape: \",df_link_subset.shape)\n",
    "        #\n",
    "        count = 0\n",
    "        for idx, row in df_link_subset.iterrows():\n",
    "            count_all_in+=1\n",
    "            count_path_all_in+=1\n",
    "            row_id = row[\"start_node_id\"]\n",
    "            col_id = row[\"end_node_id\"]\n",
    "            if dict_subset_start_count[row_id] > 1 and dict_subset_end_count[col_id] > 1:\n",
    "                list_df.append(pd.DataFrame(row).T)\n",
    "                dict_subset_start_count[row_id] = dict_subset_start_count[row_id] - 1\n",
    "                dict_subset_end_count[col_id] = dict_subset_end_count[col_id] - 1\n",
    "                count+=1\n",
    "                count_all+=1\n",
    "                count_path_all+=1\n",
    "        print(\"count: \",count)\n",
    "        print(\"#\")\n",
    "        dict_count_max[path_id][x_id] = count\n",
    "    print(\"######\")\n",
    "    print(\"before df_link.shape: \",df_link.shape)\n",
    "    if len(list_df) > 0:\n",
    "        df_link = pd.concat([df_link,pd.concat(list_df)]).drop_duplicates(keep=False)   \n",
    "    print(\"count_all: \",count_all)\n",
    "    print(\"count_all_in: \",count_all_in)\n",
    "    print(\"after df_link.shape: \",df_link.shape)\n",
    "    print(\"#######\")\n",
    "print(\"##################\")\n",
    "next_count = df_link.shape[0]\n",
    "print(\"After df_link.shape: \",df_link.shape)\n",
    "print(\"count_path_all_in: \",count_path_all_in)\n",
    "print(\"count_path_all: \",count_path_all)\n",
    "print(\"#\")\n",
    "print(\"Total removed: \",prev_count - next_count)\n",
    "print(\"##################\")\n",
    "num_edges_removed_max = count_path_all"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{(2, 0): {'X1': 7, 'X3': 263, 'X4': 1},\n",
       " (2, 1): {'X1': 0, 'X4': 0, 'X6': 23, 'X7': 1},\n",
       " (2, 2): {'X4': 0, 'X6': 0, 'X8': 0},\n",
       " (3, 0): {'X1': 2, 'X7': 6, 'X8': 0}}"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dict_count_max"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "#df_link.to_csv(out_dir_min_max+fname_link,index=False, sep=\"\\t\",header=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "############################################## min #####################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "#dict_subset_start_count = dict_subset_start_count_orig.copy()\n",
    "#dict_subset_end_count = dict_subset_end_count_orig.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "##################\n",
      "min_or_max:  min\n",
      "BEFORE df_link.shape:  (39886, 4)\n",
      "##################\n",
      "path_id:  (2, 0)\n",
      "###\n",
      "x_id:  X4\n",
      "df_link_subset.shape:  (330, 4)\n",
      "count:  247\n",
      "#\n",
      "x_id:  X3\n",
      "df_link_subset.shape:  (45, 4)\n",
      "count:  0\n",
      "#\n",
      "x_id:  X1\n",
      "df_link_subset.shape:  (0, 4)\n",
      "count:  0\n",
      "#\n",
      "######\n",
      "before df_link.shape:  (39886, 4)\n",
      "count_all:  247\n",
      "count_all_in:  375\n",
      "after df_link.shape:  (39639, 4)\n",
      "#######\n",
      "path_id:  (2, 1)\n",
      "###\n",
      "x_id:  X4\n",
      "df_link_subset.shape:  (83, 4)\n",
      "count:  0\n",
      "#\n",
      "x_id:  X6\n",
      "df_link_subset.shape:  (27, 4)\n",
      "count:  0\n",
      "#\n",
      "x_id:  X7\n",
      "df_link_subset.shape:  (1, 4)\n",
      "count:  0\n",
      "#\n",
      "x_id:  X1\n",
      "df_link_subset.shape:  (0, 4)\n",
      "count:  0\n",
      "#\n",
      "######\n",
      "before df_link.shape:  (39639, 4)\n",
      "count_all:  0\n",
      "count_all_in:  111\n",
      "after df_link.shape:  (39639, 4)\n",
      "#######\n",
      "path_id:  (2, 2)\n",
      "###\n",
      "x_id:  X4\n",
      "df_link_subset.shape:  (83, 4)\n",
      "count:  0\n",
      "#\n",
      "x_id:  X6\n",
      "df_link_subset.shape:  (27, 4)\n",
      "count:  0\n",
      "#\n",
      "x_id:  X8\n",
      "df_link_subset.shape:  (0, 4)\n",
      "count:  0\n",
      "#\n",
      "######\n",
      "before df_link.shape:  (39639, 4)\n",
      "count_all:  0\n",
      "count_all_in:  110\n",
      "after df_link.shape:  (39639, 4)\n",
      "#######\n",
      "path_id:  (3, 0)\n",
      "###\n",
      "x_id:  X8\n",
      "df_link_subset.shape:  (23, 4)\n",
      "count:  15\n",
      "#\n",
      "x_id:  X7\n",
      "df_link_subset.shape:  (2, 4)\n",
      "count:  0\n",
      "#\n",
      "x_id:  X1\n",
      "df_link_subset.shape:  (1, 4)\n",
      "count:  0\n",
      "#\n",
      "######\n",
      "before df_link.shape:  (39639, 4)\n",
      "count_all:  15\n",
      "count_all_in:  26\n",
      "after df_link.shape:  (39624, 4)\n",
      "#######\n",
      "##################\n",
      "After df_link.shape:  (39624, 4)\n",
      "count_path_all_in:  622\n",
      "count_path_all:  262\n",
      "#\n",
      "Total removed:  262\n",
      "##################\n"
     ]
    }
   ],
   "source": [
    "dict_count_min = {}\n",
    "#\n",
    "#df_link = df_link_copy2 #continue to use the same df_link with max links removed\n",
    "min_or_max = \"min\"\n",
    "print(\"##################\")\n",
    "print(\"min_or_max: \",min_or_max)\n",
    "print(\"BEFORE df_link.shape: \",df_link.shape)\n",
    "print(\"##################\")\n",
    "count_path_all = 0\n",
    "count_path_all_in = 0\n",
    "prev_count = df_link.shape[0]\n",
    "for path_id in dict_cc_edges_select:\n",
    "    print(\"path_id: \",path_id)\n",
    "    print(\"###\")\n",
    "    dict_count_min[path_id] = {}\n",
    "    cur_path_dict = dict_cc_edges_select[path_id][min_or_max]\n",
    "    #\n",
    "    #list_edges_to_filter = []\n",
    "    #list_nodes = []\n",
    "    list_df = []\n",
    "    count_all = 0\n",
    "    count_all_in = 0\n",
    "    for x_id in cur_path_dict:\n",
    "        print(\"x_id: \",x_id)\n",
    "        cur_X = data_dict[x_id]\n",
    "        cur_X_dict = cur_path_dict[x_id]\n",
    "        col_e_id = cur_X_dict[\"col_e_id\"]\n",
    "        col_node_type = int(col_e_id.replace(\"e\",\"\"))\n",
    "        row_e_id = cur_X_dict[\"row_e_id\"]\n",
    "        row_node_type = int(row_e_id.replace(\"e\",\"\"))\n",
    "        col_idx_list = list(cur_X_dict[\"col_idx_list\"])\n",
    "        row_idx_list = list(cur_X_dict[\"row_idx_list\"])\n",
    "        #obtain the corresponding row and column node ids\n",
    "#         df_row_nodes = df_map_matidx_nodeid[df_map_matidx_nodeid[\"node_type\"].isin([row_node_type])]\n",
    "#         list_row_node_id_list = df_row_nodes[df_row_nodes[\"idx\"].isin(row_idx_list)][\"node_id\"].unique().tolist()\n",
    "#         df_col_nodes = df_map_matidx_nodeid[df_map_matidx_nodeid[\"node_type\"].isin([col_node_type])]\n",
    "#         list_col_node_id_list = df_col_nodes[df_col_nodes[\"idx\"].isin(col_idx_list)][\"node_id\"].unique().tolist()\n",
    "        list_row_node_id_list = []\n",
    "        for temp_idx in row_idx_list:\n",
    "            list_row_node_id_list.append(dict_map_matidx_nodeid[row_node_type][temp_idx])\n",
    "        list_col_node_id_list = []\n",
    "        for temp_idx in col_idx_list:\n",
    "            list_col_node_id_list.append(dict_map_matidx_nodeid[col_node_type][temp_idx])\n",
    "        #\n",
    "        df_link_subset = df_link[df_link[\"start_node_id\"].isin(list_row_node_id_list)\\\n",
    "                                 & df_link[\"end_node_id\"].isin(list_col_node_id_list)]\n",
    "        print(\"df_link_subset.shape: \",df_link_subset.shape)\n",
    "        #\n",
    "        count = 0\n",
    "        for idx, row in df_link_subset.iterrows():\n",
    "            count_all_in+=1\n",
    "            count_path_all_in+=1\n",
    "            row_id = row[\"start_node_id\"]\n",
    "            col_id = row[\"end_node_id\"]\n",
    "            if dict_subset_start_count[row_id] > 1 and dict_subset_end_count[col_id] > 1:\n",
    "                list_df.append(pd.DataFrame(row).T)\n",
    "                dict_subset_start_count[row_id] = dict_subset_start_count[row_id] - 1\n",
    "                dict_subset_end_count[col_id] = dict_subset_end_count[col_id] - 1\n",
    "                count+=1\n",
    "                count_all+=1\n",
    "                count_path_all+=1\n",
    "        print(\"count: \",count)\n",
    "        print(\"#\")\n",
    "        dict_count_min[path_id][x_id] = count\n",
    "    print(\"######\")\n",
    "    print(\"before df_link.shape: \",df_link.shape)\n",
    "    if len(list_df) > 0:\n",
    "        df_link = pd.concat([df_link,pd.concat(list_df)]).drop_duplicates(keep=False)   \n",
    "    print(\"count_all: \",count_all)\n",
    "    print(\"count_all_in: \",count_all_in)\n",
    "    print(\"after df_link.shape: \",df_link.shape)\n",
    "    print(\"#######\")\n",
    "print(\"##################\")\n",
    "next_count = df_link.shape[0]\n",
    "print(\"After df_link.shape: \",df_link.shape)\n",
    "print(\"count_path_all_in: \",count_path_all_in)\n",
    "print(\"count_path_all: \",count_path_all)\n",
    "print(\"#\")\n",
    "print(\"Total removed: \",prev_count - next_count)\n",
    "print(\"##################\")\n",
    "num_edges_removed_min = count_path_all"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-1028"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "39161 - 40189"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{(2, 0): {'X1': 0, 'X3': 0, 'X4': 247},\n",
       " (2, 1): {'X1': 0, 'X4': 0, 'X6': 0, 'X7': 0},\n",
       " (2, 2): {'X4': 0, 'X6': 0, 'X8': 0},\n",
       " (3, 0): {'X1': 0, 'X7': 0, 'X8': 15}}"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dict_count_min"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{(2, 0): {'X1': 7, 'X3': 263, 'X4': 1},\n",
       " (2, 1): {'X1': 0, 'X4': 0, 'X6': 23, 'X7': 1},\n",
       " (2, 2): {'X4': 0, 'X6': 0, 'X8': 0},\n",
       " (3, 0): {'X1': 2, 'X7': 6, 'X8': 0}}"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dict_count_max"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "link_type:  0  #nodes removed:  0\n",
      "df_link_type_subset_full.shape[0]:  5142\n",
      "df_link_type_subset_rand.shape[0]:  5142\n",
      "#\n",
      "link_type:  1  #nodes removed:  9\n",
      "df_link_type_subset_full.shape[0]:  4851\n",
      "df_link_type_subset_rand.shape[0]:  4842\n",
      "#\n",
      "link_type:  2  #nodes removed:  3\n",
      "df_link_type_subset_full.shape[0]:  5732\n",
      "df_link_type_subset_rand.shape[0]:  5729\n",
      "#\n",
      "link_type:  3  #nodes removed:  276\n",
      "df_link_type_subset_full.shape[0]:  5727\n",
      "df_link_type_subset_rand.shape[0]:  5451\n",
      "#\n",
      "link_type:  4  #nodes removed:  245\n",
      "df_link_type_subset_full.shape[0]:  7494\n",
      "df_link_type_subset_rand.shape[0]:  7249\n",
      "#\n",
      "link_type:  5  #nodes removed:  22\n",
      "df_link_type_subset_full.shape[0]:  8747\n",
      "df_link_type_subset_rand.shape[0]:  8725\n",
      "#\n",
      "link_type:  6  #nodes removed:  1\n",
      "df_link_type_subset_full.shape[0]:  910\n",
      "df_link_type_subset_rand.shape[0]:  909\n",
      "#\n",
      "link_type:  7  #nodes removed:  2\n",
      "df_link_type_subset_full.shape[0]:  639\n",
      "df_link_type_subset_rand.shape[0]:  637\n",
      "#\n",
      "link_type:  8  #nodes removed:  7\n",
      "df_link_type_subset_full.shape[0]:  810\n",
      "df_link_type_subset_rand.shape[0]:  803\n",
      "#\n",
      "link_type:  9  #nodes removed:  0\n",
      "df_link_type_subset_full.shape[0]:  137\n",
      "df_link_type_subset_rand.shape[0]:  137\n",
      "#\n"
     ]
    }
   ],
   "source": [
    "dict_link_type_num_removed1 = {}\n",
    "for link_type in np.arange(num_matrices):\n",
    "    df_link_type_subset_rand = df_link[df_link[\"link_type\"].isin([link_type])]\n",
    "    df_link_type_subset_full = df_link_full[df_link_full[\"link_type\"].isin([link_type])]\n",
    "    num_removed = df_link_type_subset_full.shape[0] - df_link_type_subset_rand.shape[0]\n",
    "    print(\"link_type: \",link_type,\" #nodes removed: \",num_removed)\n",
    "    print(\"df_link_type_subset_full.shape[0]: \",df_link_type_subset_full.shape[0])\n",
    "    print(\"df_link_type_subset_rand.shape[0]: \",df_link_type_subset_rand.shape[0])\n",
    "    print(\"#\")\n",
    "    dict_link_type_num_removed1[link_type] = num_removed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{0: 0, 1: 9, 2: 3, 3: 276, 4: 245, 5: 22, 6: 1, 7: 2, 8: 7, 9: 0}"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dict_link_type_num_removed1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_link.to_csv(out_dir_min_max+fname_link,index=False, sep=\"\\t\",header=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "################################# rand 6 uniform max #############################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_pruned_df_link_uniform(df_link, dict_count_max, \\\n",
    "                        dict_subset_start_count, dict_subset_end_count):\n",
    "\n",
    "    dict_count_rand4_max = {}\n",
    "    num_edges_to_hide_threshold = 0\n",
    "    num_sampling_try_threshold = 100\n",
    "    print(\"##################\")\n",
    "    print(\"BEFORE df_link.shape: \",df_link.shape)\n",
    "    print(\"##################\")\n",
    "    count_path_all = 0\n",
    "    count_path_all_in = 0\n",
    "    prev_count = df_link.shape[0]\n",
    "    #\n",
    "    tot_num_edges_hidden_max = 200\n",
    "#     for path_id in dict_count_max:\n",
    "#         tot_num_edges_hidden_max += np.sum(list(dict_count_max[path_id].values()))\n",
    "    print(\"tot_num_edges_hidden_max: \",tot_num_edges_hidden_max)        \n",
    "    print(\"#\")\n",
    "    #\n",
    "    num_matrices = 10\n",
    "    #\n",
    "    #>>> num_edges_to_hide = int(tot_num_edges_hidden_max/num_matrices) #per matrix\n",
    "    #print(\"num_edges_to_hide, per mat: \",num_edges_to_hide)\n",
    "    #\n",
    "#     list_rand_matrices = []\n",
    "#     list_rand_matrices.append(np.random.choice(np.arange(num_matrices)))\n",
    "#     list_rand_matrices.append(np.random.choice(np.arange(num_matrices)))\n",
    "#     list_rand_matrices.append(np.random.choice(np.arange(num_matrices)))\n",
    "    for link_type in np.arange(num_matrices):\n",
    "    #for link_type in list_rand_matrices:\n",
    "        num_edges_to_hide_temp = int(tot_num_edges_hidden_max /np.random.choice(np.arange(8,10)+1))\n",
    "        num_edges_to_hide = np.random.choice([0,0,0,0,num_edges_to_hide_temp])\n",
    "        print(\"num_edges_to_hide, per mat: \",num_edges_to_hide)\n",
    "        list_df = []\n",
    "        df_link_type_subset = df_link[df_link[\"link_type\"].isin([link_type])]\n",
    "        #\n",
    "        count = 0\n",
    "        count_in = 0\n",
    "        num_sampling_try = 0   \n",
    "        while count < num_edges_to_hide:\n",
    "            cur_size = df_link_type_subset.shape[0]\n",
    "            if 3*num_edges_to_hide > cur_size:\n",
    "                df_link_subset = df_link_type_subset.sample(n=cur_size,random_state=1)\n",
    "            else:\n",
    "                df_link_subset = df_link_type_subset.sample(n=3*num_edges_to_hide,random_state=1)\n",
    "            num_sampling_try+=1\n",
    "            print(\"num_sampling_try: \",num_sampling_try)\n",
    "            for idx, row in df_link_subset.iterrows():\n",
    "                count_in+=1\n",
    "                count_path_all_in+=1\n",
    "                row_id = row[\"start_node_id\"]\n",
    "                col_id = row[\"end_node_id\"]\n",
    "                if dict_subset_start_count[row_id] > 1 and dict_subset_end_count[col_id] > 1:\n",
    "                    list_df.append(pd.DataFrame(row).T)\n",
    "                    dict_subset_start_count[row_id] = dict_subset_start_count[row_id] - 1\n",
    "                    dict_subset_end_count[col_id] = dict_subset_end_count[col_id] - 1\n",
    "                    count+=1\n",
    "                    count_path_all+=1           \n",
    "                if not count < num_edges_to_hide:\n",
    "                    break\n",
    "            if num_sampling_try > num_sampling_try_threshold:\n",
    "                break\n",
    "        #\n",
    "        if len(list_df) > 0:\n",
    "            df_link_subset = pd.concat(list_df)\n",
    "            #assert df_link_subset.shape[0] == num_edges_to_hide,\"df_link_subset.shape[0]: \"+str(df_link_subset.shape[0])+\", num_edges_to_hide: \"+str(num_edges_to_hide)\n",
    "            #\n",
    "            print(\"#\")\n",
    "            print(\"df_link_subset.shape: \",df_link_subset.shape)\n",
    "            print(\"df_link_subset.group: \")\n",
    "            pp.pprint(df_link_subset.groupby(\"link_type\").agg([\"count\"])[\"start_node_id\"])\n",
    "            print(\"#\")\n",
    "            print(\"before df_link.shape: \",df_link.shape)\n",
    "            bef_count = df_link.shape[0]\n",
    "            df_link = pd.concat([df_link,df_link_subset]).drop_duplicates(keep=False)\n",
    "            aft_count = df_link.shape[0]\n",
    "            print(\"count: \",count)\n",
    "            print(\"count_in: \",count_in)\n",
    "            print(\"after df_link.shape: \",df_link.shape)\n",
    "            print(\"#\")   \n",
    "            dict_count_rand4_max[link_type] = bef_count - aft_count\n",
    "            assert dict_count_rand4_max[link_type] <= num_edges_to_hide\n",
    "        else:\n",
    "            print(\"Skipping hiding - could not find samples that meet our criteria. \")\n",
    "    print(\"##################\")\n",
    "    next_count = df_link.shape[0]\n",
    "    print(\"AFTER df_link.shape: \",df_link.shape)\n",
    "    print(\"count_path_all_in: \",count_path_all_in)\n",
    "    print(\"count_path_all: \",count_path_all)\n",
    "    print(\"#\")\n",
    "    print(\"Total removed: \",prev_count - next_count)\n",
    "    print(\"##################\") \n",
    "    return df_link, dict_count_rand4_max"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_link = df_link_copy6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "dict_subset_start_count = dict_subset_start_count_orig.copy()\n",
    "dict_subset_end_count = dict_subset_end_count_orig.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{(2, 0): {'X1': 7, 'X3': 263, 'X4': 1},\n",
       " (2, 1): {'X1': 0, 'X4': 0, 'X6': 23, 'X7': 1},\n",
       " (2, 2): {'X4': 0, 'X6': 0, 'X8': 0},\n",
       " (3, 0): {'X1': 2, 'X7': 6, 'X8': 0}}"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dict_count_max"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "##################\n",
      "BEFORE df_link.shape:  (40189, 4)\n",
      "##################\n",
      "tot_num_edges_hidden_max:  200\n",
      "#\n",
      "num_edges_to_hide, per mat:  20\n",
      "num_sampling_try:  1\n",
      "#\n",
      "df_link_subset.shape:  (20, 4)\n",
      "df_link_subset.group: \n",
      "           count\n",
      "link_type       \n",
      "0             20\n",
      "#\n",
      "before df_link.shape:  (40189, 4)\n",
      "count:  20\n",
      "count_in:  24\n",
      "after df_link.shape:  (40169, 4)\n",
      "#\n",
      "num_edges_to_hide, per mat:  0\n",
      "Skipping hiding - could not find samples that meet our criteria. \n",
      "num_edges_to_hide, per mat:  0\n",
      "Skipping hiding - could not find samples that meet our criteria. \n",
      "num_edges_to_hide, per mat:  20\n",
      "num_sampling_try:  1\n",
      "#\n",
      "df_link_subset.shape:  (20, 4)\n",
      "df_link_subset.group: \n",
      "           count\n",
      "link_type       \n",
      "3             20\n",
      "#\n",
      "before df_link.shape:  (40169, 4)\n",
      "count:  20\n",
      "count_in:  22\n",
      "after df_link.shape:  (40149, 4)\n",
      "#\n",
      "num_edges_to_hide, per mat:  0\n",
      "Skipping hiding - could not find samples that meet our criteria. \n",
      "num_edges_to_hide, per mat:  0\n",
      "Skipping hiding - could not find samples that meet our criteria. \n",
      "num_edges_to_hide, per mat:  0\n",
      "Skipping hiding - could not find samples that meet our criteria. \n",
      "num_edges_to_hide, per mat:  0\n",
      "Skipping hiding - could not find samples that meet our criteria. \n",
      "num_edges_to_hide, per mat:  0\n",
      "Skipping hiding - could not find samples that meet our criteria. \n",
      "num_edges_to_hide, per mat:  0\n",
      "Skipping hiding - could not find samples that meet our criteria. \n",
      "##################\n",
      "AFTER df_link.shape:  (40149, 4)\n",
      "count_path_all_in:  46\n",
      "count_path_all:  40\n",
      "#\n",
      "Total removed:  40\n",
      "##################\n"
     ]
    }
   ],
   "source": [
    "#repeatedly run the below until the required number of nodes are removed\n",
    "df_link, dict_count_rand6_max_uniform = get_pruned_df_link_uniform(df_link, dict_count_max, \\\n",
    "                        dict_subset_start_count, dict_subset_end_count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1051"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "294 + 200 + 422 + 20 + 33 + 82"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-1038"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "39151 - 40189"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_link.to_csv(out_dir_rand8+fname_link,index=False, sep=\"\\t\",header=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "#how many of each node_type were removed randomly?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(40189, 4)"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_link_full.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(40149, 4)"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_link.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "link_type:  0  #nodes removed:  20\n",
      "df_link_type_subset_full.shape[0]:  5142\n",
      "df_link_type_subset_rand.shape[0]:  5122\n",
      "#\n",
      "link_type:  1  #nodes removed:  0\n",
      "df_link_type_subset_full.shape[0]:  4851\n",
      "df_link_type_subset_rand.shape[0]:  4851\n",
      "#\n",
      "link_type:  2  #nodes removed:  0\n",
      "df_link_type_subset_full.shape[0]:  5732\n",
      "df_link_type_subset_rand.shape[0]:  5732\n",
      "#\n",
      "link_type:  3  #nodes removed:  20\n",
      "df_link_type_subset_full.shape[0]:  5727\n",
      "df_link_type_subset_rand.shape[0]:  5707\n",
      "#\n",
      "link_type:  4  #nodes removed:  0\n",
      "df_link_type_subset_full.shape[0]:  7494\n",
      "df_link_type_subset_rand.shape[0]:  7494\n",
      "#\n",
      "link_type:  5  #nodes removed:  0\n",
      "df_link_type_subset_full.shape[0]:  8747\n",
      "df_link_type_subset_rand.shape[0]:  8747\n",
      "#\n",
      "link_type:  6  #nodes removed:  0\n",
      "df_link_type_subset_full.shape[0]:  910\n",
      "df_link_type_subset_rand.shape[0]:  910\n",
      "#\n",
      "link_type:  7  #nodes removed:  0\n",
      "df_link_type_subset_full.shape[0]:  639\n",
      "df_link_type_subset_rand.shape[0]:  639\n",
      "#\n",
      "link_type:  8  #nodes removed:  0\n",
      "df_link_type_subset_full.shape[0]:  810\n",
      "df_link_type_subset_rand.shape[0]:  810\n",
      "#\n",
      "link_type:  9  #nodes removed:  0\n",
      "df_link_type_subset_full.shape[0]:  137\n",
      "df_link_type_subset_rand.shape[0]:  137\n",
      "#\n"
     ]
    }
   ],
   "source": [
    "dict_link_type_num_removed = {}\n",
    "for link_type in np.arange(num_matrices):\n",
    "    df_link_type_subset_rand = df_link[df_link[\"link_type\"].isin([link_type])]\n",
    "    df_link_type_subset_full = df_link_full[df_link_full[\"link_type\"].isin([link_type])]\n",
    "    num_removed = df_link_type_subset_full.shape[0] - df_link_type_subset_rand.shape[0]\n",
    "    print(\"link_type: \",link_type,\" #nodes removed: \",num_removed)\n",
    "    print(\"df_link_type_subset_full.shape[0]: \",df_link_type_subset_full.shape[0])\n",
    "    print(\"df_link_type_subset_rand.shape[0]: \",df_link_type_subset_rand.shape[0])\n",
    "    print(\"#\")\n",
    "    dict_link_type_num_removed[link_type] = num_removed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{0: 20, 1: 0, 2: 0, 3: 20, 4: 0, 5: 0, 6: 0, 7: 0, 8: 0, 9: 0}\n"
     ]
    }
   ],
   "source": [
    "pp.pprint(dict_link_type_num_removed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{0: 0, 1: 9, 2: 3, 3: 276, 4: 245, 5: 22, 6: 1, 7: 2, 8: 7, 9: 0}"
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dict_link_type_num_removed1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
